{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\siddhanth\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\siddhanth\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\siddhanth\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\siddhanth\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\siddhanth\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Siddhanth\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vqWPVEX5SyAe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "2leGCh3wSyCn",
    "outputId": "c0eb3180-7213-4fe7-a428-5b512ae38b80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('papers.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Tas88YbXSyFb",
    "outputId": "bf16a435-c963-41d8-878c-daf40ac40dd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paper_text  index\n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...      0\n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...      1\n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...      2\n",
       "3  Bayesian Query Construction for Neural\\nNetwor...      3\n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = data.paper_text, columns = ['paper_text'], index = range(len(data)))\n",
    "df['index'] = range(len(data))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsDm6rXFSyKn",
    "outputId": "eeea41ed-0dd8-4b83-b46b-996cfc494114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cS0LEyjESyNH",
    "outputId": "23083bcd-3da0-442c-a7b4-a2bf386a9e63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper_text    0\n",
       "index         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTZkjvfQVv4O"
   },
   "source": [
    "**Preprocessing -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BrkYKLG4Dj8v"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOYqt4Z8VsNo",
    "outputId": "5dff73fa-ea52-40c7-f082-e833aef2dc34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Siddhanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Bgvn139QVsQz"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7FumY9xrVsT_"
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos = 'v'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxzPSSK6VsWv",
    "outputId": "b8af961d-12e5-4856-f6ea-80125649a673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [self, organ, associ, databas, applic, hisashi...\n",
       "1     [mean, field, theori, layer, visual, cortex, a...\n",
       "2     [store, covari, associ, long, term, potenti, d...\n",
       "3     [bayesian, queri, construct, neural, network, ...\n",
       "4     [neural, network, ensembl, cross, valid, activ...\n",
       "5     [sing, neural, instanti, deform, model, christ...\n",
       "6     [plastic, mediat, competit, learn, terrenc, se...\n",
       "7     [iceg, morpholog, classif, analogu, vlsi, neur...\n",
       "8     [real, time, control, tokamak, plasma, neural,...\n",
       "9     [real, time, control, tokamak, plasma, neural,...\n",
       "10    [learn, play, game, chess, sebastian, thrun, u...\n",
       "11    [scale, data, cluster, thoma, hofmann, joachim...\n",
       "12    [experiment, comparison, recurr, neural, netwo...\n",
       "13    [train, multilay, perceptron, extend, kalman, ...\n",
       "14    [interfer, learn, intern, model, invers, dynam...\n",
       "Name: paper_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = df['paper_text'].apply(preprocess_text)\n",
    "processed_docs[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI11Or1GFnfz"
   },
   "source": [
    "**Feature extraction through tf-idf-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HuD__UcKEZ6Q"
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wVGJvv7rEZ-C"
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3ZAbTg7BEaBR"
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sbZcdOq3EaEU"
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdRgQ7DJGDea"
   },
   "source": [
    "**Running LDA using tf-idf model -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Fu6Yt1peQTF7"
   },
   "outputs": [],
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "J91ZMeswF_4A"
   },
   "outputs": [],
   "source": [
    "lda_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = num_topics, id2word = dictionary, passes = 2, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoZtGZ-SF_7E",
    "outputId": "9b45cce3-85b2-4a89-f3f0-e2dce83bd58d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic : 0 \n",
      " Words 0.001*\"imag\" + 0.001*\"rank\" + 0.001*\"kernel\" + 0.001*\"cluster\" + 0.001*\"polici\" + 0.001*\"matrix\" + 0.001*\"latent\" + 0.001*\"posterior\" + 0.001*\"label\" + 0.001*\"train\" \n",
      "\n",
      "Topic : 1 \n",
      " Words 0.001*\"polici\" + 0.001*\"action\" + 0.001*\"reward\" + 0.001*\"neuron\" + 0.001*\"spike\" + 0.001*\"imag\" + 0.001*\"network\" + 0.001*\"agent\" + 0.001*\"kernel\" + 0.001*\"train\" \n",
      "\n",
      "Topic : 2 \n",
      " Words 0.001*\"kernel\" + 0.001*\"imag\" + 0.000*\"layer\" + 0.000*\"convex\" + 0.000*\"network\" + 0.000*\"classifi\" + 0.000*\"lasso\" + 0.000*\"theorem\" + 0.000*\"train\" + 0.000*\"featur\" \n",
      "\n",
      "Topic : 3 \n",
      " Words 0.001*\"regret\" + 0.001*\"imag\" + 0.001*\"cluster\" + 0.001*\"label\" + 0.001*\"bandit\" + 0.001*\"polici\" + 0.001*\"queri\" + 0.001*\"rank\" + 0.001*\"reward\" + 0.001*\"kernel\" \n",
      "\n",
      "Topic : 4 \n",
      " Words 0.001*\"kernel\" + 0.001*\"imag\" + 0.001*\"cluster\" + 0.001*\"polici\" + 0.001*\"graph\" + 0.001*\"label\" + 0.001*\"network\" + 0.001*\"manifold\" + 0.001*\"layer\" + 0.001*\"train\" \n",
      "\n",
      "Topic : 5 \n",
      " Words 0.002*\"neuron\" + 0.001*\"spike\" + 0.001*\"imag\" + 0.001*\"layer\" + 0.001*\"cluster\" + 0.001*\"network\" + 0.001*\"kernel\" + 0.001*\"synaps\" + 0.001*\"cell\" + 0.001*\"graph\" \n",
      "\n",
      "Topic : 6 \n",
      " Words 0.001*\"kernel\" + 0.000*\"cluster\" + 0.000*\"imag\" + 0.000*\"network\" + 0.000*\"rank\" + 0.000*\"trajectori\" + 0.000*\"train\" + 0.000*\"loss\" + 0.000*\"unit\" + 0.000*\"graph\" \n",
      "\n",
      "Topic : 7 \n",
      " Words 0.001*\"neuron\" + 0.001*\"spike\" + 0.001*\"kernel\" + 0.001*\"network\" + 0.001*\"matrix\" + 0.001*\"layer\" + 0.001*\"rank\" + 0.001*\"train\" + 0.001*\"imag\" + 0.000*\"polici\" \n",
      "\n",
      "Topic : 8 \n",
      " Words 0.001*\"cluster\" + 0.001*\"graph\" + 0.001*\"imag\" + 0.000*\"polici\" + 0.000*\"neuron\" + 0.000*\"regret\" + 0.000*\"kernel\" + 0.000*\"submodular\" + 0.000*\"network\" + 0.000*\"theorem\" \n",
      "\n",
      "Topic : 9 \n",
      " Words 0.001*\"cluster\" + 0.001*\"topic\" + 0.001*\"imag\" + 0.001*\"kernel\" + 0.001*\"network\" + 0.001*\"word\" + 0.001*\"neuron\" + 0.001*\"train\" + 0.001*\"layer\" + 0.001*\"graph\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    print(f\"Topic : {idx} \\n Words {topic} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aihpZE_bktdf"
   },
   "source": [
    "**Common words between each pair of topics-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTVhS54DRGwt",
    "outputId": "c4323639-aeea-4668-9921-84c314ce507a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0.001*\"imag\" + 0.001*\"rank\" + 0.001*\"kernel\" + 0.001*\"cluster\" + 0.001*\"polici\" + 0.001*\"matrix\" + 0.001*\"latent\" + 0.001*\"posterior\" + 0.001*\"label\" + 0.001*\"train\"',\n",
       " 1: '0.001*\"polici\" + 0.001*\"action\" + 0.001*\"reward\" + 0.001*\"neuron\" + 0.001*\"spike\" + 0.001*\"imag\" + 0.001*\"network\" + 0.001*\"agent\" + 0.001*\"kernel\" + 0.001*\"train\"',\n",
       " 2: '0.001*\"kernel\" + 0.001*\"imag\" + 0.000*\"layer\" + 0.000*\"convex\" + 0.000*\"network\" + 0.000*\"classifi\" + 0.000*\"lasso\" + 0.000*\"theorem\" + 0.000*\"train\" + 0.000*\"featur\"',\n",
       " 3: '0.001*\"regret\" + 0.001*\"imag\" + 0.001*\"cluster\" + 0.001*\"label\" + 0.001*\"bandit\" + 0.001*\"polici\" + 0.001*\"queri\" + 0.001*\"rank\" + 0.001*\"reward\" + 0.001*\"kernel\"',\n",
       " 4: '0.001*\"kernel\" + 0.001*\"imag\" + 0.001*\"cluster\" + 0.001*\"polici\" + 0.001*\"graph\" + 0.001*\"label\" + 0.001*\"network\" + 0.001*\"manifold\" + 0.001*\"layer\" + 0.001*\"train\"',\n",
       " 5: '0.002*\"neuron\" + 0.001*\"spike\" + 0.001*\"imag\" + 0.001*\"layer\" + 0.001*\"cluster\" + 0.001*\"network\" + 0.001*\"kernel\" + 0.001*\"synaps\" + 0.001*\"cell\" + 0.001*\"graph\"',\n",
       " 6: '0.001*\"kernel\" + 0.000*\"cluster\" + 0.000*\"imag\" + 0.000*\"network\" + 0.000*\"rank\" + 0.000*\"trajectori\" + 0.000*\"train\" + 0.000*\"loss\" + 0.000*\"unit\" + 0.000*\"graph\"',\n",
       " 7: '0.001*\"neuron\" + 0.001*\"spike\" + 0.001*\"kernel\" + 0.001*\"network\" + 0.001*\"matrix\" + 0.001*\"layer\" + 0.001*\"rank\" + 0.001*\"train\" + 0.001*\"imag\" + 0.000*\"polici\"',\n",
       " 8: '0.001*\"cluster\" + 0.001*\"graph\" + 0.001*\"imag\" + 0.000*\"polici\" + 0.000*\"neuron\" + 0.000*\"regret\" + 0.000*\"kernel\" + 0.000*\"submodular\" + 0.000*\"network\" + 0.000*\"theorem\"',\n",
       " 9: '0.001*\"cluster\" + 0.001*\"topic\" + 0.001*\"imag\" + 0.001*\"kernel\" + 0.001*\"network\" + 0.001*\"word\" + 0.001*\"neuron\" + 0.001*\"train\" + 0.001*\"layer\" + 0.001*\"graph\"'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    topic_dict[idx] = topic\n",
    "\n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "T35zLBDNF_-G"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_hd0xJlzRZPm"
   },
   "outputs": [],
   "source": [
    "def topic_words(topic):\n",
    "    topics = re.findall(\"\\D{4,}\", topic)\n",
    "    return list(map(lambda x: x[2: x.find('\"', x.find('\"') + 1)], topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejCtg68ZGAAx",
    "outputId": "bab1c3e0-0d6a-465d-96cb-78f0a1b4171f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 and 1 : 4\n",
      "Common words - {'train', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 0 and 2 : 3\n",
      "Common words - {'kernel', 'train', 'imag'}\n",
      "\n",
      "\n",
      "Topic 0 and 3 : 6\n",
      "Common words - {'label', 'rank', 'cluster', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 0 and 4 : 6\n",
      "Common words - {'label', 'cluster', 'train', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 0 and 5 : 3\n",
      "Common words - {'cluster', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 0 and 6 : 5\n",
      "Common words - {'rank', 'cluster', 'train', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 0 and 7 : 6\n",
      "Common words - {'rank', 'matrix', 'train', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 0 and 8 : 4\n",
      "Common words - {'cluster', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 0 and 9 : 4\n",
      "Common words - {'kernel', 'cluster', 'train', 'imag'}\n",
      "\n",
      "\n",
      "Topic 1 and 2 : 4\n",
      "Common words - {'network', 'train', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 1 and 3 : 4\n",
      "Common words - {'imag', 'reward', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 1 and 4 : 5\n",
      "Common words - {'train', 'network', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 1 and 5 : 5\n",
      "Common words - {'spike', 'neuron', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 1 and 6 : 4\n",
      "Common words - {'network', 'train', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 1 and 7 : 7\n",
      "Common words - {'spike', 'train', 'neuron', 'network', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 1 and 8 : 5\n",
      "Common words - {'neuron', 'network', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 1 and 9 : 5\n",
      "Common words - {'train', 'neuron', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 2 and 3 : 2\n",
      "Common words - {'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 2 and 4 : 5\n",
      "Common words - {'layer', 'train', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 2 and 5 : 4\n",
      "Common words - {'network', 'layer', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 2 and 6 : 4\n",
      "Common words - {'network', 'train', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 2 and 7 : 5\n",
      "Common words - {'layer', 'train', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 2 and 8 : 4\n",
      "Common words - {'network', 'theorem', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 2 and 9 : 5\n",
      "Common words - {'layer', 'train', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 3 and 4 : 5\n",
      "Common words - {'label', 'cluster', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 3 and 5 : 3\n",
      "Common words - {'cluster', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 3 and 6 : 4\n",
      "Common words - {'kernel', 'cluster', 'rank', 'imag'}\n",
      "\n",
      "\n",
      "Topic 3 and 7 : 4\n",
      "Common words - {'kernel', 'rank', 'imag', 'polici'}\n",
      "\n",
      "\n",
      "Topic 3 and 8 : 5\n",
      "Common words - {'regret', 'cluster', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 3 and 9 : 3\n",
      "Common words - {'cluster', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 4 and 5 : 6\n",
      "Common words - {'graph', 'layer', 'cluster', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 4 and 6 : 6\n",
      "Common words - {'graph', 'cluster', 'train', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 4 and 7 : 6\n",
      "Common words - {'layer', 'train', 'network', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 4 and 8 : 6\n",
      "Common words - {'graph', 'cluster', 'network', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 4 and 9 : 7\n",
      "Common words - {'graph', 'layer', 'cluster', 'train', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 5 and 6 : 5\n",
      "Common words - {'graph', 'cluster', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 5 and 7 : 6\n",
      "Common words - {'layer', 'spike', 'neuron', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 5 and 8 : 6\n",
      "Common words - {'graph', 'cluster', 'neuron', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 5 and 9 : 7\n",
      "Common words - {'graph', 'layer', 'cluster', 'neuron', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 6 and 7 : 5\n",
      "Common words - {'rank', 'train', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 6 and 8 : 5\n",
      "Common words - {'graph', 'cluster', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 6 and 9 : 6\n",
      "Common words - {'graph', 'cluster', 'train', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 7 and 8 : 5\n",
      "Common words - {'neuron', 'network', 'imag', 'kernel', 'polici'}\n",
      "\n",
      "\n",
      "Topic 7 and 9 : 6\n",
      "Common words - {'layer', 'train', 'neuron', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n",
      "Topic 8 and 9 : 6\n",
      "Common words - {'graph', 'cluster', 'neuron', 'network', 'imag', 'kernel'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for combination in combinations(list(range(10)), 2):\n",
    "    common_words = set(topic_words(topic_dict[combination[0]])).intersection(set(topic_words(topic_dict[combination[1]])))\n",
    "    print(f\"Topic {combination[0]} and {combination[1]} : {len(common_words)}\", end = \"\\n\")\n",
    "    print(f\"Common words - {common_words}\", end = \"\\n\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEcBydg3lokC"
   },
   "source": [
    "**Evaluating the topic model -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48zZou3YWG6m",
    "outputId": "f807a67c-564b-4479-d470-7723e5d2b1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.21785028975304832\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_tfidf, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKrqJOr8n88P"
   },
   "source": [
    "Higher the coherence score , better it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sAtUVnroh2D"
   },
   "source": [
    "**Making a different model to get a higher coherence score -**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4VQnTfiubqb"
   },
   "source": [
    "Increasing the number of passes as well as the chunksize - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "I5bxKv4EWHOx"
   },
   "outputs": [],
   "source": [
    "lda_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = num_topics, id2word = dictionary, passes = 15, workers = 4, \n",
    "                                       per_word_topics = True, chunksize = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7679UDO2WHRt",
    "outputId": "43947bf8-66ef-4b2b-fbc5-7453c36f487e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic : 0 \n",
      " Words 0.000*\"minwis\" + 0.000*\"eld\" + 0.000*\"gamp\" + 0.000*\"wedg\" + 0.000*\"abstent\" + 0.000*\"nnbm\" + 0.000*\"romma\" + 0.000*\"acnn\" + 0.000*\"diabolo\" + 0.000*\"recollect\" \n",
      "\n",
      "Topic : 1 \n",
      " Words 0.000*\"mwis\" + 0.000*\"ltsa\" + 0.000*\"miso\" + 0.000*\"mcboost\" + 0.000*\"cann\" + 0.000*\"oasi\" + 0.000*\"epitom\" + 0.000*\"ggood\" + 0.000*\"adex\" + 0.000*\"lpboost\" \n",
      "\n",
      "Topic : 2 \n",
      " Words 0.000*\"mimo\" + 0.000*\"lissom\" + 0.000*\"pointset\" + 0.000*\"epic\" + 0.000*\"gnkr\" + 0.000*\"omdp\" + 0.000*\"srcut\" + 0.000*\"lsdd\" + 0.000*\"gibbsnet\" + 0.000*\"adclus\" \n",
      "\n",
      "Topic : 3 \n",
      " Words 0.000*\"churn\" + 0.000*\"mirna\" + 0.000*\"forgeri\" + 0.000*\"diskmean\" + 0.000*\"isomer\" + 0.000*\"glas\" + 0.000*\"vamp\" + 0.000*\"href\" + 0.000*\"gape\" + 0.000*\"sgns\" \n",
      "\n",
      "Topic : 4 \n",
      " Words 0.000*\"flanker\" + 0.000*\"mmsb\" + 0.000*\"zeta\" + 0.000*\"kcca\" + 0.000*\"gpfa\" + 0.000*\"lgssm\" + 0.000*\"vine\" + 0.000*\"conv_\" + 0.000*\"decontamin\" + 0.000*\"dapt\" \n",
      "\n",
      "Topic : 5 \n",
      " Words 0.002*\"kernel\" + 0.002*\"imag\" + 0.002*\"neuron\" + 0.002*\"cluster\" + 0.002*\"polici\" + 0.002*\"network\" + 0.001*\"spike\" + 0.001*\"train\" + 0.001*\"layer\" + 0.001*\"graph\" \n",
      "\n",
      "Topic : 6 \n",
      " Words 0.000*\"lifelong\" + 0.000*\"bumptre\" + 0.000*\"logloss\" + 0.000*\"cuboid\" + 0.000*\"plds\" + 0.000*\"endmemb\" + 0.000*\"arrhythmia\" + 0.000*\"ebnn\" + 0.000*\"glimps\" + 0.000*\"cssp\" \n",
      "\n",
      "Topic : 7 \n",
      " Words 0.000*\"knng\" + 0.000*\"psrl\" + 0.000*\"cvae\" + 0.000*\"plasma\" + 0.000*\"ssda\" + 0.000*\"xopt\" + 0.000*\"sghmc\" + 0.000*\"pbay\" + 0.000*\"micl\" + 0.000*\"walkback\" \n",
      "\n",
      "Topic : 8 \n",
      " Words 0.000*\"rpca\" + 0.000*\"hyperedg\" + 0.000*\"mixabl\" + 0.000*\"blossom\" + 0.000*\"missing\" + 0.000*\"hyperkernel\" + 0.000*\"grbf\" + 0.000*\"bisimul\" + 0.000*\"mplp\" + 0.000*\"vmos\" \n",
      "\n",
      "Topic : 9 \n",
      " Words 0.000*\"rivalri\" + 0.000*\"subband\" + 0.000*\"strf\" + 0.000*\"knudsen\" + 0.000*\"spikernel\" + 0.000*\"clime\" + 0.000*\"sonn\" + 0.000*\"tectum\" + 0.000*\"dikernel\" + 0.000*\"elegan\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    print(f\"Topic : {idx} \\n Words {topic} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NcTB8q3IWHUk"
   },
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    topic_dict[idx] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQBAcADRWHXZ",
    "outputId": "1b5d3d5f-71c6-4755-f9e9-4504639331db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 and 1 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 2 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 3 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 4 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 5 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 6 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 7 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 0 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 2 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 3 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 4 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 5 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 6 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 7 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 1 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 2 and 3 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 2 and 4 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 2 and 5 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 2 and 6 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 2 and 7 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 2 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 2 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 3 and 4 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 3 and 5 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 3 and 6 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 3 and 7 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 3 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 3 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 4 and 5 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 4 and 6 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 4 and 7 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 4 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 4 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 5 and 6 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 5 and 7 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 5 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 5 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 6 and 7 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 6 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 6 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 7 and 8 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 7 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n",
      "Topic 8 and 9 : 0\n",
      "Common words - set()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for combination in combinations(list(range(10)), 2):\n",
    "    common_words = set(topic_words(topic_dict[combination[0]])).intersection(set(topic_words(topic_dict[combination[1]])))\n",
    "    print(f\"Topic {combination[0]} and {combination[1]} : {len(common_words)}\", end = \"\\n\")\n",
    "    print(f\"Common words - {common_words}\", end = \"\\n\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXIezk45pG47",
    "outputId": "3f04da33-fe30-4db7-b2b3-c99366752bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.6227777987406713\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_tfidf, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QZan-KkpHU3"
   },
   "source": [
    "## less no of common words and high scores show that the topics have very little in common"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "J009_NLP_Asst2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
